---
title: Workflow parameterisation
teaching: 20
exercises: 5
---

::::::::::::::::::::::::::::::::::::::: objectives

- Use pipeline parameters to change the input to a workflow.
- Add pipeline parameters to a Nextflow script.
- Understand how to create and use a parameter file.

::::::::::::::::::::::::::::::::::::::::::::::::::

:::::::::::::::::::::::::::::::::::::::: questions

- How can I change the data a workflow uses?
- How can I parameterise a workflow?
- How can I add my parameters to a file?

::::::::::::::::::::::::::::::::::::::::::::::::::

In the first episode we ran the Nextflow script, `read_data.nf`, from the
command line and it de-compressed the archive folder that contained data on 6 individual schools for two time points
`each_period.tar.gz`. To change the input to script we can make use of pipeline parameters.

## Pipeline parameters

The Nextflow `read_data.nf` script defines a pipeline parameter `params.input`.
Pipeline parameters enable you to change the input to the workflow at
runtime, via the command line or a configuration file, so they are not
hard-coded into the script.

Pipeline parameters are declared in the workflow by prepending the
prefix `params`, separated by the dot character, to a variable name
e.g., `params.input`.

Their value can be specified on the command line by prefixing the
parameter name with a **double dash** character, e.g., `--input`.

In the script `read_data.nf` the pipeline parameter `params.input` was
specified with a value of `"each_period.tar.gz"`.

To process a different file, e.g. `multi_period.tar.gz`, in
the `read_data.nf` script we would run:

``` bash
nextflow run read_data.nf --input 'multi_period.tar.gz'
```

``` output
 N E X T F L O W   ~  version 24.10.4

Launching `read_data.nf` [loving_brenner] DSL2 - revision: 8a3d1bb9c7

executor >  local (1)
executor >  local (1)
[49/214249] process > GENERATE_READS (1) [100%] 1 of 1 ✔
[/workspaces/training/work/49/21424945038a3a509a67cf9d092711/school123.RDS, 
/workspaces/training/work/49/21424945038a3a509a67cf9d092711/school124.RDS, 
/workspaces/training/work/49/21424945038a3a509a67cf9d092711/school125.RDS, 
/workspaces/training/work/49/21424945038a3a509a67cf9d092711/school126.RDS, 
/workspaces/training/work/49/21424945038a3a509a67cf9d092711/school127.RDS, 
/workspaces/training/work/49/21424945038a3a509a67cf9d092711/school128.RDS]
```

We can also use wild cards to specify multiple input files (This will be
covered in the channels episode). In the example below we use the `*` to
match any sequence of characters before `multi_period.tar.gz`. **Note:**
If you use wild card characters on the command line you must enclose the
value in quotes.

``` bash
nextflow run read_data.nf --input '*multi_period.tar.gz'
```

This runs the process NUM_LINES twice, once for each file it matches.

``` output
 N E X T F L O W   ~  version 24.10.4

Launching `read_data.nf` [grave_hopper] DSL2 - revision: 8a3d1bb9c7

executor >  local (2)
[5f/7df89f] process > GENERATE_READS (2) [100%] 2 of 2 ✔
[/workspaces/training/work/df/253fc08b9b2941144e0e67c8e3c213/school123.dat, 
/workspaces/training/work/df/253fc08b9b2941144e0e67c8e3c213/school124.dat, 
/workspaces/training/work/df/253fc08b9b2941144e0e67c8e3c213/school125.dat, 
/workspaces/training/work/df/253fc08b9b2941144e0e67c8e3c213/school126.dat, 
/workspaces/training/work/df/253fc08b9b2941144e0e67c8e3c213/school127.dat, 
/workspaces/training/work/df/253fc08b9b2941144e0e67c8e3c213/school128.dat]

[/workspaces/training/work/5f/7df89f35cb0de22fe9eb6c91e833ed/school123.RDS, 
/workspaces/training/work/5f/7df89f35cb0de22fe9eb6c91e833ed/school124.RDS, 
/workspaces/training/work/5f/7df89f35cb0de22fe9eb6c91e833ed/school125.RDS, 
/workspaces/training/work/5f/7df89f35cb0de22fe9eb6c91e833ed/school126.RDS, 
/workspaces/training/work/5f/7df89f35cb0de22fe9eb6c91e833ed/school127.RDS, 
/workspaces/training/work/5f/7df89f35cb0de22fe9eb6c91e833ed/school128.RDS]

```

:::::::::::::::::::::::::::::::::::::::  challenge

## Change a pipeline's input using a parameter

Re-run the Nextflow script `read_data.nf` by changing the pipeline input to all
files in the directory that end
with `each_period.tar.gz`:

:::::::::::::::  solution

## Solution

``` bash
nextflow run read_data.nf --input '*each_period.tar.gz'
```

The string specified on the command line will override the default value
of the parameter in the script. The output will look like this:

``` output

 N E X T F L O W   ~  version 24.10.4

Launching `read_data.nf` [lethal_cajal] DSL2 - revision: 8a3d1bb9c7

executor >  local (2)
[05/8e0aa0] process > GENERATE_READS (1) [100%] 2 of 2 ✔
[/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school123_period1.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school123_period2.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school124_period1.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school124_period2.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school125_period1.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school125_period2.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school126_period1.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school126_period2.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school127_period1.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school127_period2.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school128_period1.dat, 
/workspaces/training/work/05/8e0aa09cc3795d1a3fc2ed1384adf7/school128_period2.dat]

executor >  local (2)
[05/8e0aa0] process > GENERATE_READS (1) [100%] 2 of 2 ✔

[/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school123_period1.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school123_period2.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school124_period1.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school124_period2.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school125_period1.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school125_period2.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school126_period1.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school126_period2.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school127_period1.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school127_period2.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school128_period1.RDS, 
/workspaces/training/work/07/303a7d7f5a8a582db4d9df86d68a08/school128_period2.RDS]

```

:::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::::::::::::::::


## Specifying an output directory in the script

To specify an output directory for a script use the parameter `publishDir` in the definition of a process.

:::::::::::::::::::::::::::::::::::::::  challenge

## Change a pipeline's output directory 

Let's make a copy of the `read_data.nf` script as `read_data_params.nf` and add specify the output directory, where we want Nextflow to store data files that were the result of decompressing the `tar` archive folder.

:::::::::::::::  solution

``` bash
cp read_data.nf read_data_params.nf
```

The files that are extracted from the archive are stored in the folder `work` that tracks the different tasks that are launched as part of the pipeline. If we want a local copy of files that are decompressed we can add an output directory in the `read_data_params.nf`:

``` groovy

  publishDir "$projectDir", mode: "copy", overwrite: true

```

**Note:** You should always add a sensible default value to the pipeline
parameter. 

``` groovy
process GENERATE_READS {

  input:
          path targz

  publishDir "$projectDir", mode: "copy", overwrite: true

  output:
          path "*"

  script:
          """
          tar -xzf $targz
          # Print file name
          printf '${targz}\\t'
          """
}
```
:::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::::::::::::::::

This step, `publishDir "$projectDir"`, will add a directory to output the decompressed files. The set of options that can be specified are listed in the [Nextflow documentation](https://www.nextflow.io/docs/latest/reference/stdlib.html). To access the value inside the process definition we use `$parameter` syntax e.g. `$projectDir`.

We can now change the sleep parameter from the command line. 
Example:

``` groovy
process GENERATE_READS {
.
.
.
publishDir "$projectDir", mode: "copy", overwrite: true
.
.
.
}
```

:::::::::::::::::::::::::::::::::::::::  challenge


## Add a pipeline parameter

If you haven't already make a copy of the `read_data.nf` as `read_data_params.nf`.

```bash
$ cp read_data.nf read_data_params.nf
```

Add the output directory `"$projectDir/tmp"` in place of the `publishDir` specification. 
Replace the `"$projectDir"` in the process `GENERATE_READS` with `"$projectDir/tmp"`.

Run the new script `read_data_params.nf` changing the output publishDir.

```bash
mkdir tmp
```

What output folder would it specify and why?

:::::::::::::::  solution

## Solution

``` groovy
process GENERATE_READS {
.
.
.
publishDir "$projectDir/tmp", mode: "copy", overwrite: true
.
.
.
}
```

This would use `publishDir "$projectDir/tmp"` parameter instead of default `publishDir "$projectDir"` and run the pipeline. 

```bash
nextflow run read_data_params.nf 
```

:::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::::::::::::::::

## Parameter File

If we have many parameters to pass to a script it is best to create a
parameters file. Parameters can be stored in JSON format. JSON is a
data serialization language, that is a way of storing data
objects and structures, such as the `params` object in a file.

The `-params-file` option is used to pass the parameters file to the
script.

For example the file `wc-params.json` contains the parameters `sleep`
and `input` in JSON format.

```json         
{
  "sleep": 5,
  "input": "data/yeast/reads/etoh60_1*.fq.gz"
}
```

Create a file called `wc-params.json` with the above contents.

To run the `wc-params.nf` script using these parameters we add the
option `-params-file` and pass the file `wc-params.json`:

```bash         
$ nextflow run wc-params.nf -params-file wc-params.json
```



```output         
N E X T F L O W  ~  version 21.04.0
Launching `wc-params.nf` [nostalgic_northcutt] - revision: 2f86c9ac7e
executor >  local (2)
[b4/747eaa] process > NUM_LINES (1) [100%] 2 of 2 ✔
etoh60_1_2.fq.gz 87348

etoh60_1_1.fq.gz 87348
```


:::::::::::::::::::::::::::::::::::::::  challenge

## Create and use a Parameter file.

Create a parameter file `params.json` for the Nextflow file
`wc-params.nf`, and run the Nextflow script using the created
parameter file, specifying:

-   sleep as 10
-   input as `data/yeast/reads/ref3_1.fq.gz`

:::::::::::::::  solution

## Solution

```json         
{
"sleep": 10,
"input": "data/yeast/reads/ref3_1.fq.gz"
}
```
```bash
$ nextflow run wc-params.nf -params-file params.json 
```

```output
N E X T F L O W 
 version 21.04.0 Launching `wc-params.nf` [small_wiles] - revision:
f5ef7b7a01 executor \> local (1) [f3/4fa480] process \> NUM_LINES
(1) [100%] 1 of 1 ✔ ref3_1.fq.gz 52592 
```
:::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::::::::::::::::


:::::::::::::::::::::::::::::::::::::::: keypoints
- Pipeline parameters are specified by prepending the prefix `params` to a variable name, separated by dot character.
- To specify a pipeline parameter on the command line for a Nextflow run use `--variable_name` syntax.
- You can add parameters to a JSON formatted file and pass them to the script using option `-params-file`.
::::::::::::::::::::::::::::::::::::::::::::::::::
