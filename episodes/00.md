---
title: "FAIR research software"
teaching: 15
exercises: 10
---

:::::::::::::::::::::::::::::::::::::: questions 

- What are the FAIR research principles?
- How do FAIR principles apply to software?
- How should I name my files?
- How does folder organisation help me
 
::::::::::::::::::::::::::::::::::::::::::::::::  <!-- Closing tag added here -->

::::::::::::::::::::::::::::::::::::: objectives
After completing this episode, participants should be able to:

- Explain the FAIR research principles in the context of research software
- Explain why these principles are of value in the research community 
- Understand elements of good naming strategy
- Evaluate pros and cons of different project organizations
- Explain how file management helps in being FAIR

::::::::::::::::::::::::::::::::::::::::::::::::

Let's explore a subset of good software practices based on the FAIR principles.

## What is FAIR?

FAIR stands for Findable, Accessible, Interoperable, and Reusable and comprises a set of principles designed to
increase the visibility and usefulness of your research to others.
The FAIR data principles, first published [in 2016][fair-data-principles], are widely known and applied today.
Similar [FAIR principles for software][fair-principles-research-software] have now been defined too. In general, they mean:

- **Findable** - software and its associated metadata must be easy to discover by humans and machines.
- **Accessible** - in order to reuse software, the software and its metadata must be retrievable by standard protocols, free and legally usable.
- **Interoperable** - when interacting with other software it must be done by exchanging data and/or metadata through
  standardised protocols and application programming interfaces (APIs).
- **Reusable** - software should be usable (can be executed) and reusable
  (can be understood, modified, built upon, or incorporated into other software).

Each of the above principles can be achieved by a number of practices listed below.
This is not an exact science, and by all means the list below is not exhaustive,
but any of the practices that you employ in your research software workflow will bring you
closer to the gold standard of fully reproducible research.

### Findable
- Create a description of your software to make it discoverable by search engines and other search tools
- Use standards (such as [CodeMeta][codemeta]) to describe interoperable metadata for your software (see [Research Software Metadata Guidelines][rsmd-g1])
- Place your software in a public software repository (and ideally register it in a [general-purpose or domain-specific software registry][awesome-rs-registries])
- Use a unique and persistent identifier (DOI) for your software (e.g. by depositing your code on [Zenodo][zenodo]),
  which is also useful for citations - note that depositing your data/code on GitHub and similar software repositories
  may not be enough as they may change their open access model or disappear completely in the future, so archiving your code means it stands a better chance at being preserved

### Accessible
- Make sure people can obtain get a copy your software using standard communication protocols (e.g. HTTP(S), (S)FTP, etc.)
- The code and its description (metadata) has to be available even when the software is no longer actively developed (this includes earlier versions of the software)

### Interoperable
- Use community-agreed standard formats for inputs and outputs of your software and its metadata
- Communicate with other software and tools via standard protocols and APIs

### Reusable
- Document your software (including its functionality, how to install and run it) so it is both usable (can be executed) 
and reusable (can be understood, modified, built upon, or incorporated into other software)
- Give a licence to your software clearly stating how it can be reused

<iframe src="https://padlet.com/padlets/43u5pibszgy94bti/embeds/preview_embed" style="width:100%;height:100%;display:block;padding:0;margin:0" frameborder="0"></iframe>

:::::: callout

## FAIR is a process, not a perfect metric

FAIR is not a binary metric - there is no such thing as "FAIR or "not FAIR".

FAIR is not a perfect metric, nor does it provide a full and exhaustive software quality checklist - there are other 
good software quality practices not covered by FAIR. 
Conversely, software may be FAIR but still not very good in terms of its functionality.

FAIR is **not meant** to criticise or discredit work. 

FAIR refers to the specific **values** of and describes a set of **principles** to aid open and reproducible research
that can be a helpful guide for researchers who want to improve their practices (by helping them see where they are 
on the **FAIR spectrum** and help them on a **journey** to make their software more FAIR). 

::::::

We are going to explore the FAIR and other good software quality practices on an example software project we will be working on as part of this course.

:::  challenge

Think of a piece of software you use in your research - any computational tool used for data gathering, modelling & simulation, processing & visualising results or others. 
If you have a bit of code or software you wrote yourself, in any language, feel free to use that.

Think where on the FAIR spectrum it fits, using the following scale as a guide for each principle:

- 1 - requires loads of improvement
- 2 - on a good path, but improvements still needed
- 3 - decent, a few things could still be improved
- 4 - very good, only tiny things to improve upon
- 5 - excellent

::::::::::::::::::::::::::::::::::::::::::::::::


::::::::::::::::::::::::::::::::::::: discussion

Look at our software project compare its data and code to the software you chose earlier (or assess 
it on its own).
Do you think it is Findable, Accessible, Interoperable and Reusable? 
Give it a score from 1 to 5 in each category, as in the previous exercise, and then we will discuss it together.

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: hint

Here are some questions to help you assess where on the FAIR spectrum the code is:

1. **Findable**
  * If these files were emailed to you, or sent on a chat platform, or handed to you on a memory stick, how easy would it be to find them again in 6 months, or 3 years?
  * If you asked your collaborator to give you the files again later on, how would you describe them? Do they have a clear name? 
  * If more data was added to the data set later, could you explain exactly which data you used in the original analysis?
2. **Accessible**
  * If the person who gave you the files left your institution, how would you get access to the files again?
  * Once you have the files, can you understand the code? Does it make sense to you?
  * Do you need to log into anything to use this? Does it require purchase or subscription to a service, platform or tool?
3. **Interoperable**
  * Is it clear what kind of input data it can read and what kind of output data is produced? Will you be able to create the input files and read the output files with the tools your community generally uses? 
  * If you wanted to use this tool as part of a larger data processing pipeline, does it allow you to link it with other tools in standard ways such as an API or command-line interface?
4. **Reusable**
  * Can you run the code on your platform/operating system (is there documentation that covers installation instructions)? What programs or libraries do you need to install to make it work (and which versions)? Are these commonly used tools in your field?
  * Do you have explicit permission to use your collaborators code in your own research and do they expect credit of some form (paper authorship, citation or acknowledgement)? Are you allowed to edit, publish or share the files with others?
  * Is the language used familiar to you and people in your research field? Can you read the variable names in the code and the column names in the data file and understand what they mean?
  * Is the code written in a way that allows you to easily modify or extend it? Can you easily see what parameters to change to make it calculate a different statistic, or run on a different input file?

::::::::::::::::::::::::::::::::::::::::::::::::

:::  solution

I would give the following scores:

F - 1/5

  - Positive: None
  - Negative: No descriptive name, identifier or version number. No way to find again except through one person and they might not remember what file you mean.

A - 2/5

  - Positive: No accounts or paid services needed. Python is free, the data is free and under a shareable license
  - Negative: No way to get the code without that one person.  Not clear where the data comes or what license it has unless you check the URL in the comment.

I - 3/5

  - Positive: CSV and JSON files are common and well documented formats. They are machine- and human-readable. They could be generated by or fed into other programs in a pipeline.
  - Negative: JSON might not be well used in some fields. No API or CLI.

R - 2/5

  - Positive: Can ask collaborator for explicit permissions for using and modifying and how to credit them, if they did not specify before. Python is a common language.
  - Negative: Python and library versions not specified. Bad variable names, hardcoded inputs, no clear structure or documentation.

::::::::::::::::::::::::::::::::::::::::::::::::

Let's now have a look into tools and practices that are commonly used in research that can help us develop software in a more FAIR way.

Some things to take into account to decide on your naming convention are:

- Does your convention make your files easy to sort
and find (e.g. by important features)?
- Include parameters that are as descriptive as possible 
	(i.e.: project, experiment, researcher, sample, organism,
	date/range, data type, method).
- Defined a standard vocabulary (shortcuts) for parameters and
  document any abbreviation.
- Decide which elements go in which order.
- Decide the convention when to use symbols, capitals, hyphens
(e.g kebab-case, CamelCase, or snake_case).
- Define a maximum name length. Aim for filenames no longer than ~30 characters.

**Do's:**

- for dates use the YYYY-MM-DD standard and place at the end of the file UNLESS you need to organize your files chronologically
- include version number (if applicable), use leading zeroes (i.e.: v005 instead of v5).
- make sure the 3-letter file format extension is present at the end of the name (e.g. .doc, .xls, .mov, .tif)
- add a **PROJECT_STRUCTURE** (README) file in your top directory which details your naming convention, directory structure and abbreviations

**Don'ts:**

- avoid using spaces (use _ or - instead)
- avoid dots, commas and special characters (e.g. ~ ! @ # $ % ^ & * ( ) ` ; < > ? , [ ] { } ‘ “)
- avoid using language specific characters (e.g óężé), unfortunately
they still cause problems with most software or between operating systems (OS)
- avoid long names
- avoid repetition, e.g if directory name is *Electron_Microscopy_Images*,
  and file *ELN_MI_IMG_20200101.img* then ELN_MI_IMG is redundant
- avoid deep paths with long names (i.e. deeply nested folders with long names)
as archiving or moving between OS may fail

:::::::::::::::::::::::::::::::::::::::::::::::: challenge

 ## Exercise 2: A good name (3 min)

 Select which file options adhere the best to the presented recommendations:
 
 1.  
 a) analysis-20210906.xlsx  
 b) rna-levels-by-site.v002.xlsx  
 c) analysis of rna levels from 5Aug2021.xlsx
 
 2.  
 a) 20210906-birds-count-EDI.csv  
 b) birds.csv  
 c) birds-count&diversity EDI 2021-09-06.csv
 
 3.  
 a) 2020-7-12_s2_phyB\_+_SD_t01.raw.xlsx  
 b) ld_phyA_on_s02-t01_2020-07-12.norm.xlsx  
 c) ld_phya_ons_02-01_2020-07-12.norm.xlsx
 
:::::::::::::::::::::::::::::::::::::::::::::::: solution
## Solution

 * 1 b)
 * 2 a)
 * 3 b)

:::::::::::::::::::::::::::::::::::::::::::::::: 
:::::::::::::::::::::::::::::::::::::::::::::::: 

If adding all the relevant details to file names makes them too long,
it is often a signal that you should use folders to organize the files and
capture some of those parameters.

:::::::::::::::::::::::::::::::::::::::::::::::: challenge

## Exercise 3: Folders vs Files 
(5 min)

 Have a look as these two different organization strategies:

 (1)
 |-- Project  
 |-- |-- arab_LD_phyA_off_t04_2020-08-12.metab.xlsx

 (2)
 |-- Project  
 |-- |-- arabidopsis  
 |-- |-- |-- long_day  
 |-- |-- |-- |-- phyA  
 |-- |-- |-- |-- |-- off_sucrose_2020-08-12  
 |-- |-- |-- |-- |-- |-- t04.metab.xlsx

 Can you think of scenarios in which one is better suited than other?
 Hint: think of other files that could be present as well.

:::::::::::::::::::::::::::::::::::::::::::::::: solution

## Solution
 The first strategies, can work very well if the project has only few files,
 so all of them can quickly be accessed (no need to change folders) and
 the different parameters are easily visible.
 For example a couple of conditions, couple of genotypes or species

 |-- Project  
 |-- |-- arab_LD_phyA_off_t04_2020-08-12.metab.xlsx  
 |-- |-- arab_LD_WILD_off_t03_2020-08-11.metab.xlsx  
 |-- |-- arab_SD_phyA_off_t01_2020-05-12.metab.xlsx  
 |-- |-- arab_SD_WILD_off_t02_2020-05-11.metab.xlsx  
 |-- |-- rice_LD_phyA_off_t05_2020-05-02.metab.xlsx  
 |-- |-- rice_LD_WILD_off_t06_2020-05-02.metab.xlsx  
 |-- |-- rice_SD_phyA_off_t07_2020-06-02.metab.xlsx  
 |-- |-- rice_SD_WILD_off_t08_2020-06-02.metab.xlsx  

 The second strategy works better if we have a lot of individual files for
 each parameter.
 For example, imagine the metabolites are measured hourly throughout the day,
 and there are ten different genotypes, two species and 4 light conditions.
 You would not want to have all the 2000 files in one folder.

 |-- Project  
 |-- |-- arabidopsis  
 |-- |-- |-- long_day  
 |-- |-- |-- |-- phyA  
 |-- |-- |-- |-- |-- off_sucrose_2020-08-12  
 |-- |-- |-- |-- |-- |-- t01.metab.xlsx  
 |-- |-- |-- |-- |-- |-- t02.metab.xlsx  
 |-- |-- |-- |-- |-- |-- t03.metab.xlsx  
 |-- |-- |-- |-- |-- |--     ...  
 |-- |-- |-- |-- |-- |-- t23.metab.xlsx  
 |-- |-- |-- |-- |-- |-- t24.metab.xlsx  
 |-- |-- rice  
 |-- |-- |-- long_day  
 |-- |-- |-- |-- phyA  
 |-- |-- |-- |-- |-- off_sucrose_2020-06-03  
 |-- |-- |-- |-- |-- |-- t01.metab.xlsx  
 |-- |-- |-- |-- |-- |--     ...  
 |-- |-- |-- |-- |-- |-- t24.metab.xlsx  
  
:::::::::::::::::::::::::::::::::::::::::::::::: 
::::::::::::::::::::::::::::::::::::::::::::::::

(4 min teaching)

## Must do: Document your strategy
 Regardless of whether you are using long filenames or incorporating
 some of the variables within the folder structure, document it!  
 Always include a PROJECT_STRUCTURE (or README) file describing your file naming and folder organisation conventions.



## Strategies to set up a clear folder structure 
(3 min teaching)

Establishing a system that allows you to access your files,
avoid duplication and ensure that your data can be easily found
needs planning.

You can start by developing a logical folder structure. To do so, you need to take into account the following suggestions:

- Use folders to group related files. A single folder will make it easy to locate them.
- Name folders appropriately: use descriptive names after the areas of work to which they relate.
- Structure folders hierarchically: use broader topics for your main folders and increase in specificity as you go down the hierarchy.
- Be consistent: agree on a naming convention from the outset of your research project.


:::::::::::::::::::::::::::::::::::::::::::::::: challenge

## Exercise 4: Typical folder organizations 
(5 min breakout + 7 explanation)

 Have a look at the four different folder structures. 
 <img src="./fig/07-file_organisation.png" alt="file-organisation-strategies" width="600"/>
 *Figure credits: Ines Boehm*  

 The first two: a) b) are recommended for computing, the other two: c) d) are for more wet/biological projects.

 * Which one is the most similar to your project structure?  
 * When/why would you use a) and when/why b)
 * When/why would you use c) and when/why d)

:::::::::::::::::::::::::::::::::::::::::::::::: solution

## Solution
 Firstly, the root directory contains a README file that provides an overview of the project as a whole,
 a CITATION file that explains how to reference it and a LICENSE, all three make it **REUSABLE**.

 The **a)** structure is recommended by the [Good enough practices in scientific computing](https://doi.org/10.1371/journal.pcbi.1005510) paper.  
 This project structure clearly separates the inputs (the raw data)
 from the outputs (the results) and the analysis procedure (python code).
 Following the same convention (like src folder for code) makes it easy
 to find interesting elements, for example the raw data or particular plotting procedure.
 Good for sharing analysis project, also for pipelines where one set of inputs generated the set of outputs in the step by step manner.

 The **b)** structure is called "Organized by analysis" or "by figure".
 As the name suggest it may be recommended to share data underling a publication. In that way each paper figure is represented by its raw data, processing scripts
 and the final results and figure plots. It is also well suited if each analysis deals with different data type or different aspect of it.
 When compared to 'a)' it makes easier to match the required inputs with the computational procedures.

 The structure similar to c) is recommended for Brain Imaging Data Structure [BIDS](https://doi.org/10.1038/sdata.2016.44), as it is organized by "patient" 
 (in this case patient was replaced by pig :) ) and type of scans. Here the focus is on individual subject / samples, for which various data was obtained.

 Structured **d)** is useful when we are interested in outcomes of experimental conditions (here drug treatments). The same set of samples/subjects/genotypes are exposed to different experimental variables/conditions and the data are probably compared between all the samples at the same conditions.

:::::::::::::::::::::::::::::::::::::::::::::::: 
::::::::::::::::::::::::::::::::::::::::::::::::

(4 min teaching)
:::::::::::::::::::::::::::::::::::::::::::::::: spoiler

## (Optional) Good enough practices for scientific computing recommendations

 The [Good enough practices in scientific computing](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec009) paper makes the following simple recommendations:

 * Put each project in its own directory, which is named after the project
 * Put text documents associated with the project in the 'doc' directory
 * Put raw data and metadata in a 'data' directory
 * Put files generated during cleanup and analysis in a 'results' directory
 * Put project source code in the 'src' directory
 * Put compiled programs in the 'bin' directory
 * Name all files to reflect their content or function:
 * Use names such as 'bird_count_table.csv', 'notebook.md', or 'summarized_results.csv'.
 * Do not use sequential numbers (e.g., result1.csv, result2.csv) or a location in a final manuscript (e.g., fig_3_a.png), since those numbers will almost certainly change as the project evolves.

::::::::::::::::::::::::::::::::::::::::::::::: 


## Tools for assessing FAIRness

Here are some tools that can check your software and provide an assessment of its FAIRness:

- [FAIRsoft evaluator][fair-rs-evaluator]
- [FAIR software test][fair-rs-test]
- [`How FAIR is your software` - command line tool to evaluate a software repository's compliance with the FAIR principles][howfairis]


## Further reading

We recommend the following resources for some additional reading on the topic of this episode:

- ["Five recommendations for FAIR software"][5-fair-software-recommendations]
- ["10 easy things to make your research software FAIR"][10-easy-fair-things]
- ["Ten simple rules for training scientists to make better software"][10-rules-better-software]
- [Automating assessment of the FAIR Principles for Research Software (FAIR4RS)][automated-assessment-fairrs]
- [Short online courses][nesc-rs-support-courses] on various aspects of research software 
(including [FAIR research software][nesc-rs-support-course-fair] and data), by the NeSC Research Software Support
- [CodeRefinery][coderefinery] - training and e-Infrastructure for research software development
- A [self-assessment checklist for FAIR research software][fair-rs-checklist], by the Netherlands eScience Center
    and Australian Research Data Commons 
- [Awesome Research Software Registries][awesome-rs-registries] - a list of research software 
registries (by country, organisation, domain and programming language) where research software can be registered 
to help promote its discovery

Also check the [full reference set](learners/reference.md#litref) for the course.



:::::::::::::::::::::::::::::::::::::::: keypoints

- Open research means the outputs of publicly funded research are publicly accessible with no or minimal restrictions.
- Reproducible research means the data and software is available to recreate the analysis.
- FAIR data and software is Findable, Accessible, Interoperable, Reusable.
- These principles support research and researchers by saving time, reducing barriers to discovery, and increasing impact of the research output.


::::::::::::::::::::::::::::::::::::::::::::::::::

## Reproducibility 


More information on these definitions can be found in "Reproducibility vs. Replicability: A Brief History of a Confused Terminology" by Hans E. Plesser {cite:ps}`Plesser2018Reproducibility`.

![Grid with the characteristics of: Reproducible; same data, same analysis. Replicable; different data, same analysis. Robust; same data, different analysis. And generalisable; different data, different analysis; Research](/episodes/fig/reproducible-definition.svg)

*_The Turing Way_ project illustration by Scriberia. Used under a CC-BY 4.0 licence. DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807)*


## FAIR principles

The FAIR guiding principles for scientific data management and stewardship {cite:ps}`Wilkinson2016fair` were developed as guidelines to improve the **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability of digital assets; all of which support research reproducibility.

In brief, FAIR data should be:

**Findable:** The first step in (re)using data is to find it!
Descriptive metadata (information about the data such as keywords) is essential.

**Accessible:** Once the user finds the data and software they need to know how to access it.
Data could be openly available but it is also possible that authentication and authorisation procedures are necessary.

**Interoperable:** Data needs to be integrated with other data and interoperate with applications or workflows.

**Reusable:** Data should be well-described so that they can be used, combined, and extended in different settings.

It is much easier to make data FAIR if you plan to do this from the beginning of your research project.
You can plan for this in your Data Management Plan (DMP) (see points 4 and 5 of the {ref}`Data Management Plan<rr-rdm-dmp>` chapter).

Even though the FAIR principles have been defined to allow machines to find and use digital objects automatically, they improve the reusability of data by humans as well.
The capacity of computational systems to find, access, interoperate, and reuse data, with none or minimal human intervention, is essential in today's data-driven era, where humans increasingly rely on computational support to deal with data as a result of the increase in [volume, velocity and
variety](https://www.zdnet.com/article/volume-velocity-and-variety-understanding-the-three-vs-of-big-data/).

This chapter provides an abstract and broad view of what the FAIR principles are. How to put the FAIR principles into practise is discussed in other sub chapters ( {ref}`Data Organisation in Spreadsheets<rr-rdm-fair>`, {ref}`Documentation and Metadata<rr-rdm-metadata>` and {ref}`Sharing and Archiving Data<rr-rdm-sharing>`). You can also use the [Wellcome Getting Started Guide](https://f1000researchdata.s3.amazonaws.com/resources/FAIR_Open_GettingStarted.pdf) or the [How To FAIR](https://howtofair.dk/) website to find out more about the FAIR principles and how to get started.


You can find a more detailed [overview of the FAIR principles by GO FAIR](https://www.go-fair.org/fair-principles) of what the FAIR principles recommend.
You can also read [A FAIRy tale](https://doi.org/10.5281/zenodo.2248200) for an understandable explanation of each principle.

Making data 'FAIR' is not the same as making it 'open'.
Accessible means that there is a procedure in place to access the data.
Data should be as open as possible and as closed as necessary.

It is also important to say that the FAIR principles are aspirational: they do not strictly define how to achieve a state of FAIRness, but rather describe a continuum of features, attributes, and behaviours that will move a digital resource closer to that goal.

The FAIR principles are also applied to software (see [[LGK+20](https://book.the-turing-way.org/afterword/bibliography.html#id10)]and [[HCH+20](https://book.the-turing-way.org/afterword/bibliography.html#id9)]). Watch a [ten minute video on FAIR software](https://www.youtube.com/watch?v=ME8_NRGRhSs&list=PL1CvC6Ez54KDvJbbdLn5rPvf1kInifEh9&index=16) for a short explanation.

## FAIR principles and environmental sustainability

> "FAIR practices can result in highly efficient code implementations, reduce the need to retrain models, and reduce unnecessary data generation/storage, thus reducing the overall carbon footprint.
> As a result, green computing and FAIR practices may both stimulate innovation and reduce financial costs." - {cite:ps}`Lannelongue2023greener`

[Estimate your projects' carbon footprint](https://calculator.green-algorithms.org/)

## FAIR principles and accessibility

The Accessible in FAIR is not equal to ensuring that your research objects are accessibles to all users. 
For this, the term “actually accessible” has been coined by {cite:ps}`Colon2023accessibility` to refer to data that is "easy to locate, obtain, interpret, use, share, and analyze for everybody, including disabled people."

(rr-rdm-fair-community)=
## Community involvement

Various online resources are provided for people who are working in the life sciences, to guide them in ensuring FAIRness in their data, providing them with tools and advice for good data management at various stages of their work. Two prominent ones include: 
* Under the [FAIR Cookbook](https://faircookbook.elixir-europe.org/content/home.html), several resources are offering guidance and assistance in FAIR data management.
 The FAIR Cookbook is designed to serve a variety of audience types and involved in different stages of data management life cycle.
The FAIR Cookbook is developed and maintained by life sciences professionals, both in the academia and industry sectors, including members of the ELIXIR community. 
* Under [ELIXIR Research Data Management Kit (RDMkit)](https://rdmkit.elixir-europe.org/), resources are provided for life scientists to guide them in better management of their research data in adhering to the FAIR Principles. 
It is an attempt to help researchers work at different capacities, both in individual and collaborative workspaces.
The RDMkit is open for suggestions from anyone, as long as they abide by the [contributor responsibilities](https://rdmkit.elixir-europe.org/how_to_contribute).



Many groups and organisations are working to define guidance and tools to help researchers and other stakeholders (like librarians, funders, publishers, and trainers) make data more FAIR.
There are two global initiatives that act as umbrella organisations and reference points for many discipline-specific efforts, including the ones listed above: [GOFAIR](https://www.go-fair.org) and the [Research Data Alliance (RDA)](https://www.rd-alliance.org).
* Under GOFAIR, there are many [Implementation Networks (INs)](https://www.go-fair.org/implementation-networks) committed to implementing the FAIR principles.
* Under the RDA, there are several groups tackling different aspects relevant to the RDM life cycle. Among these, one group, the [FAIR Data Maturity Model Working Group](https://www.rd-alliance.org/groups/fair-data-maturity-model-wg) is reviewing existing efforts, building on them to define a standard set of common assessment criteria for the evaluation of FAIRness.


Big data is conceptualised in different ways by different researchers.
"Big" data may be complex, come from a variety of data sources, is large in storage volume and/or be streamed at very high temporal resolution.
Although there are ways to set random seeds and take snapshots of a dataset at a particular moment in time, it can be difficult to have identical data across different runs of an analysis pipeline.
This is particularly relevant in the context of tools for parallel computing.
For example, some data such as flight tracking or internet traffic is so big that it can not be stored and must be processed as it is streamed in real time.

A more common challenge for "big data" researchers is the variability of software performance across operating systems and how quickly the tools change over time.
An almost constantly changing ecosystem of data science technologies is available, which means reproducing results in the future is highly variable and dependent on using perfectly backwards compatible tools as they develop.
Very often the results of statistical tests will vary depending on the configuration of the infrastructure that was used in each of the experiments, making it very hard to independently reproduce a result.
Experiments are often dependent on random initialisation for iterative algorithms and not all software includes the ability to fix a pseudorandom number without limiting parallelisation capabilities (for example in Tensorflow).
These tools can require in depth technical skills which are not widely available to data scientists.
The [Apache Hadoop](https://hadoop.apache.org/) framework, for instance, is extremely complex to deploy data science experiments without strong software and hardware engineering knowledge.

Even "standard" high performance computing, can be difficult to set up to be perfectly reproducible, particularly across different cloud computing providers or institutional configurations.
_The Turing Way_ contains chapters to help data scientists learn skills in {ref}`reproducible computational environments<rr-renv>` including {ref}`containers<rr-renv-containers>` such as docker and ways to {ref}`version control your software libraries<rr-renv-package>`.
We are always [open to more contributions](https://github.com/the-turing-way/the-turing-way/blob/main/CONTRIBUTING.md) as the technology to support reproducible research in very large datasets or for complex modelling evolves.

## Takes time

Making an analysis reproducible takes time and effort, particularly at the start of the project.
This may include agreeing upon a {ref}`testing framework<rr-testing>`, setting up {ref}`version control<rr-vcs>` such as a Github repository and {ref}`continuous integration<rr-ci>`, and {ref}`managing data<rr-rdm>`.
Throughout the project, time may be required to maintain the reproducible pipeline.

Time may also be spent communicating with collaborators to agree on which parts of the project may be open source and when and how these outputs are shared.

The analysis pipeline can be easily adapted as needed in response to co-author and reviewer requests.
It can also be easily reused for future research projects.

This barrier isn't really a _barrier_ to reproducible research as much as a caveat that investing time in reproducibility doesn't necessarily mean that you're doing better science.
You can consider computational reproducibility as being necessary but not sufficient for high quality research.
A critical approach is needed, rather than naively using existing software or implementing statistical methods without understanding what they do.

## Held to higher standards than others

```{figure} ../../figures/make-ok-to-be-human.*
---
height: 500px
name: make-ok-to-be-human
alt: A cartoon of a woman holding a folder of files and looking worried. Thought bubble says, If I share my data people might find mistakes. The caption on the images reads Need to make it ok to be human.
---
An illustration of the "plead the fifth" barrier where our current culture disincentivises acknowledging and correcting mistakes.
Illustration by The Ludic Group LLP from Kirstie Whitaker's keynote presentation at Scientific Data in 2017.
Used under a CC-BY 4.0 license.
DOI: [10.6084/m9.figshare.5577340.v1](https://doi.org/10.6084/m9.figshare.5577340.v1).
```

A researcher who makes their work reproducible by sharing their code and data may be held to a higher standard than other researchers.
If authors share nothing at all, then all readers of a manuscript or conference paper can do is trust (or not trust) the results.

If code and data are available, peer reviewers may go looking for differences in the implementation.
They may come back with new ideas on ways to analyse the data because they have been able to experiment with the work.
There is a risk that they then require additional changes from the authors of the submitted manuscript before it is accepted for peer review.


![alt text](../../workflows-nextflow/episodes/fig/FAIR.svg)

![alt text](../../workflows-nextflow/episodes/fig/version-control.svg)

![alt text](../../workflows-nextflow/episodes/fig/reproducible-definition.svg)

![alt text](../../workflows-nextflow/episodes/fig/reproducibility-idea.jpg)

![alt text](../../workflows-nextflow/episodes/fig/reproducibility.jpg)

![alt text](../../workflows-nextflow/episodes/fig/open-science.png)

![alt text](../../workflows-nextflow/episodes/fig/open-hardware.jpg)

![alt text](../../workflows-nextflow/episodes/fig/make-ok-to-be-human.png)

